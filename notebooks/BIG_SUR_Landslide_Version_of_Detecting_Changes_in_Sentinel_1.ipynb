{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mthomp89/landslide-detect/blob/main/BIG_SUR_Landslide_Version_of_Detecting_Changes_in_Sentinel_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kdsGkYJXXKc"
   },
   "outputs": [],
   "source": [
    "#@title Copyright 2020 The Earth Engine Community Authors { display-mode: \"form\" }\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l18M9_r5XmAQ"
   },
   "source": [
    "# Detecting Changes in Sentinel-1 Imagery For Verified Lanslide Locations \n",
    "\n",
    "Authors: Leah Manak and Mitch Thompson.\n",
    "Special credit to: Elsa Culler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7i55vr_aKCB"
   },
   "source": [
    "### Run me first\n",
    "\n",
    "Run the following cell to initialize the API. The output will contain instructions on how to grant this notebook access to Earth Engine using your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeFsiSp2aDL6",
    "outputId": "68002968-4b38-4a92-ca65-f31c13d77d59"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "# Trigger the authentication flow.\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the library.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOf_UnIcZKBJ"
   },
   "source": [
    "### Datasets and Python modules\n",
    "One [dataset](https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S1_GRD) that will be used in the tutorial is:\n",
    "\n",
    "- COPERNICUS/S1_GRD_FLOAT\n",
    "    - Sentinel-1 ground range detected images\n",
    "\n",
    "Another is a verified landslide locations dataset created by CU Boulder Earth Lab that will be defined below as \"landslide_df\". \n",
    "- This dataset includes various verified locations of landslides across North America with descriptions of severity and type. \n",
    "The following cell imports some python modules which we will be using as we go along and enables inline graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmusFZcZHEjE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import earthpy as et\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gamma, f, chi2\n",
    "import IPython.display as disp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZEVxUZ2mTSe"
   },
   "source": [
    "This cell carries over the chi square cumulative distribution function and the determinant of a Sentinel-1 image from [Part 2](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li189P8wmOTS"
   },
   "outputs": [],
   "source": [
    "def chi2cdf(chi2, df):\n",
    "    \"\"\"Calculates Chi square cumulative distribution function for\n",
    "       df degrees of freedom using the built-in incomplete gamma\n",
    "       function gammainc().\n",
    "    \"\"\"\n",
    "\n",
    "    return ee.Image(chi2.divide(2)).gammainc(ee.Number(df).divide(2))\n",
    "\n",
    "def det(im):\n",
    "    \"\"\"Calculates determinant of 2x2 diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    return im.expression('b(0)*b(1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eelxHh2qc6xg"
   },
   "source": [
    "To make use of interactive graphics, we import the _folium_ package:\n",
    "- here we may also import geemap as well if we choose to make an interactive split map before/after landslide event. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEiSY5zdoFPe"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def add_ee_layer(self, ee_image_object, vis_params, name):\n",
    "    \"\"\"Adds Earth Engine layers to a folium map.\"\"\"\n",
    "    \n",
    "    map_id_dict = ee.Image(ee_image_object).getMapId(vis_params)\n",
    "    folium.raster_layers.TileLayer(\n",
    "        tiles = map_id_dict['tile_fetcher'].url_format,\n",
    "        attr = 'Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
    "        name = name,\n",
    "        overlay = True,\n",
    "        control = True).add_to(self)\n",
    "\n",
    "# Add EE drawing method to folium.\n",
    "folium.Map.add_ee_layer = add_ee_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set working directory\n",
    "I created a folder in our \"earth-analytics\" folder that is titled \"landslide-detect\". Here, we can add our datasets so we can both access them the same way through this notebook. \n",
    "-We still need to find a way to have this be more reproducible. There should be a way to have this csv on Git Hub for future downloads so we don't have to do it manually on our own computer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to landslide-detect data path\n",
    "data_path = os.path.join(et.io.HOME, \"earth-analytics\", \"landslide-detect\")\n",
    "if os.path.exists(data_path):\n",
    "    os.chdir(data_path)\n",
    "else:\n",
    "    os.makedirs(data_path)\n",
    "    print('The new directory is created!')\n",
    "    os.chdir(data_path)\n",
    "\n",
    "print('Current working directory is set to: ', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Pandas Dataframe of the verified landslides\n",
    "This dataframe is from the landslides.verified.csv. We are currently adding the minx, miny, maxx, maxy values as a \"bounding box\" for the locations, however this should be overwritten with a 1km buffer using BBox class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CSV and Create DataFrame with Pandas\n",
    "landslide_df = pd.read_csv(\"landslides.verified.csv\", index_col=\"slide.id\")\n",
    "# \n",
    "landslide_df['minx'] = landslide_df.apply(lambda row: row.lon -.01, axis=1)\n",
    "landslide_df['maxx'] = landslide_df.apply(lambda row: row.lon +.01, axis=1)\n",
    "landslide_df['miny'] = landslide_df.apply(lambda row: row.lat -.01, axis=1)\n",
    "landslide_df['maxy'] = landslide_df.apply(lambda row: row.lat +.01, axis=1)\n",
    "\n",
    "landslide_df\n",
    "landslide_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folium Map of All Verified Landslide Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_locations = landslide_df[[\"lat\", \"lon\", \"type\"]]\n",
    "\n",
    "mp = folium.Map(\n",
    "    location=[verified_locations.lat.mean(), verified_locations.lon.mean()],\n",
    "    zoom_start=4,\n",
    "    control_scale=True,\n",
    "    tiles=\"Stamen Terrain\")\n",
    "\n",
    "for index, location_info in landslide_df.iterrows():\n",
    "    folium.Marker([location_info[\"lat\"], location_info[\"lon\"]], popup=location_info[\"type\"]).add_to(mp)\n",
    "\n",
    "display(mp) \n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXo28Rx8HTEd"
   },
   "source": [
    "## Multitemporal change detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d43UObDY-hjL"
   },
   "source": [
    "In order to accurately detect land-change due to lanslides, we need to gather information across a time series to determine the changes that occured and to get accurate location information for the landslides. To get started, we need to create a time series for each location.\n",
    "\n",
    "### A time series\n",
    "For now, we are creating the geoJSON bounding box by hand for our initial study sites: Big Sur and Wyoming.\n",
    "\n",
    "The following code will be for both Big Sur and Wyoming loctions of landslides that occured in 2017. The boundaries for each location are identified, and the coordinate marker on the folium map are the \"verified\" locations identified from our database.\n",
    "- Since we are just experimenting now on what two sites will look like, we have the same code copied for both sites, thus one needs to be commented out while running the code to avoid errors. We will need to only have one geoJSON version at the end that will run all locations cited in the folium map above, however for now we have a separate value for both sites. Each place in the code will be marked as either the Big Sur site or the Wyoming site for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Big Sur site id #9734\n",
    "# geoJSON = {\n",
    "#   \"type\": \"FeatureCollection\",\n",
    "#   \"features\": [\n",
    "#     {\n",
    "#       \"type\": \"Feature\",\n",
    "#       \"properties\": {},\n",
    "#       \"geometry\": {\n",
    "#         \"type\": \"Polygon\",\n",
    "#         \"coordinates\": [\n",
    "\n",
    "#           [\n",
    "#             [\n",
    "#               -121.42,\n",
    "#               35.86\n",
    "#             ],\n",
    "#             [\n",
    "#               -121.44,\n",
    "#               35.86\n",
    "#             ],\n",
    "#             [\n",
    "#               -121.44,\n",
    "#               35.87\n",
    "#             ],\n",
    "#             [\n",
    "#               -121.42,\n",
    "#               35.87,\n",
    "#             ],\n",
    "#             [\n",
    "#               -121.42,\n",
    "#               35.86\n",
    "#             ]\n",
    "#           ]\n",
    "#         ]\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "\n",
    "# coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "# aoi = ee.Geometry.Polygon(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyoming geoJSON example (site id #9806)\n",
    "geoJSON = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [\n",
    "\n",
    "          [\n",
    "            [\n",
    "              -108.40,\n",
    "              43.60\n",
    "            ],\n",
    "            [\n",
    "              -108.00,\n",
    "              43.60\n",
    "            ],\n",
    "            [\n",
    "              -108.00,\n",
    "              43.40\n",
    "            ],\n",
    "            [\n",
    "              -108.40,\n",
    "              43.40,\n",
    "            ],\n",
    "            [\n",
    "              -108.40,\n",
    "              43.60\n",
    "            ]\n",
    "          ]\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "coords = geoJSON['features'][0]['geometry']['coordinates']\n",
    "aoi = ee.Geometry.Polygon(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big Sur AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BIG SUR/ make sure the geoJSON for BIG SUR is not commented out to run this code:\n",
    "# m = folium.Map(location=[35.865628, -121.4323838],\n",
    "#                zoom_start=15, tiles=\"Stamen Terrain\")\n",
    "# folium.Marker([35.865628, -121.4323838],\n",
    "#               popup=\"<i>Big Sur Landslide</i>\").add_to(m)\n",
    "# folium.GeoJson(geoJSON).add_to(m)\n",
    "# # This GeoJson addition below is the bounding box for bit\n",
    "# # m.add_ee_layer(aoi)\n",
    "# display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wyoming AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F5NXLcern6ff",
    "outputId": "380c4aa8-6752-4cef-a463-4da141e2e461"
   },
   "outputs": [],
   "source": [
    "# Wyoming:\n",
    "m = folium.Map(location=[43.51364004, -108.1852766],\n",
    "               zoom_start=11, tiles=\"Stamen Terrain\")\n",
    "folium.Marker([43.51364004, -108.1852766],\n",
    "              popup=\"<i>Wyoming Landslide</i>\").add_to(m)\n",
    "folium.GeoJson(geoJSON).add_to(m)\n",
    "# This GeoJson addition below is the bounding box for bit\n",
    "# m.add_ee_layer(aoi)\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqvC_m12kZ6X"
   },
   "source": [
    "## Big Sur Image Collection\n",
    "The image collection below covers the months of September, 2016 through September, 2017 at 6-day intervals. This allows for enough timestamps for us to determine land-change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W317cgEc_uhH",
    "outputId": "7133dbb9-7004-4176-8876-d54ed69c85e5"
   },
   "outputs": [],
   "source": [
    "# im_coll = (ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "#            .filterBounds(aoi)\n",
    "#            .filterDate(ee.Date('2016-09-01'), ee.Date('2017-09-01'))\n",
    "#            .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "#            .filter(ee.Filter.eq('relativeOrbitNumber_start', 42))\n",
    "#            .filter(ee.Filter.listContains('transmitterReceiverPolarisation',\n",
    "#                                           'VV'))\n",
    "#            .filter(ee.Filter.listContains('transmitterReceiverPolarisation',\n",
    "#                                           'VH'))\n",
    "#            .map(lambda img: img.set('date',\n",
    "#                                     ee.Date(img.date()).format('YYYYMMdd')))\n",
    "#            .sort('date'))\n",
    "\n",
    "# timestamplist = (im_coll.aggregate_array('date')\n",
    "#                  .map(lambda d: ee.String('T').cat(ee.String(d)))\n",
    "#                  .getInfo())\n",
    "# timestamplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyoming Image Collection\n",
    "The image collection below covers the months of September, 2016 through September, 2017 at 6-day intervals. This allows for enough timestamps for us to determine land-change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_coll = (ee.ImageCollection('COPERNICUS/S1_GRD_FLOAT')\n",
    "           .filterBounds(aoi)\n",
    "           .filterDate(ee.Date('2016-09-01'), ee.Date('2017-09-01'))\n",
    "           .filter(ee.Filter.eq('orbitProperties_pass', 'DESCENDING'))\n",
    "           .filter(ee.Filter.eq('relativeOrbitNumber_start', 27))\n",
    "           .filter(ee.Filter.listContains('transmitterReceiverPolarisation',\n",
    "                                          'VV'))\n",
    "           .filter(ee.Filter.listContains('transmitterReceiverPolarisation',\n",
    "                                          'VH'))\n",
    "           .map(lambda img: img.set('date',\n",
    "                                    ee.Date(img.date()).format('YYYYMMdd')))\n",
    "           .sort('date'))\n",
    "\n",
    "timestamplist = (im_coll.aggregate_array('date')\n",
    "                 .map(lambda d: ee.String('T').cat(ee.String(d)))\n",
    "                 .getInfo())\n",
    "timestamplist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUiTi-eynGPi"
   },
   "source": [
    "## Convert and Clip Image Collection\n",
    "Convert the image collection to a list and, clip the images to our AOI for both sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOCbUmW-UHIK",
    "outputId": "1b33bc9d-20ce-4b6d-8a71-7ec3edc39d9a"
   },
   "outputs": [],
   "source": [
    "# # BIG Sur \n",
    "# def clip_img(img):\n",
    "#     \"\"\"Clips a list of images.\"\"\"\n",
    "#     return ee.Image(img).clip(aoi)\n",
    "\n",
    "# im_list = im_coll.toList(im_coll.size())\n",
    "\n",
    "\n",
    "# im_list = ee.List(im_list.map(clip_img))\n",
    "# im_list.get(0)\n",
    "# ee.Image(im_list.get(0)).bandNames().getInfo()\n",
    "# im_list.length().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyoming\n",
    "def clip_img(img):\n",
    "    \"\"\"Clips a list of images.\"\"\"\n",
    "    return ee.Image(img).clip(aoi)\n",
    "\n",
    "im_list = im_coll.toList(im_coll.size())\n",
    "\n",
    "\n",
    "im_list = ee.List(im_list.map(clip_img))\n",
    "im_list.get(0)\n",
    "ee.Image(im_list.get(0)).bandNames().getInfo()\n",
    "im_list.length().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectvv(current):\n",
    "    return ee.Image(current).select('VV')\n",
    "\n",
    "vv_list = im_list.map(selectvv)\n",
    "\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "mp = folium.Map(location=location, zoom_start=11)\n",
    "rgb_images = (ee.Image.rgb(vv_list.get(9), vv_list.get(10), vv_list.get(11))\n",
    "              .log10().multiply(10))\n",
    "mp.add_ee_layer(rgb_images, {'min': -20,'max': 0}, 'rgb composite')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnsgS-VVs6rS"
   },
   "source": [
    "## Now we have a series of 22 SAR images for Big Sur and 32 for Wyoming, and we would like to know where and when changes have taken place.\n",
    "\n",
    "\n",
    "Well, one problem is the rate of false positives. If the bitemporal tests are statistically independent, then the probability of **not** getting a false positive over a series of length $k$ is the product of not getting one in each of the $k-1$ intervals, i.e., $(1-\\alpha)^{k-1}$ and the overall first kind error probability $\\alpha_T$ is its complement:\n",
    "\n",
    "$$\n",
    "\\alpha_T = 1-(1-\\alpha)^{k-1}. \\tag{3.1}\n",
    "$$\n",
    "\n",
    "For our case, even with a small value of $\\alpha=0.01$, this gives a whopping 22.2% false positive rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkDk-YKthoA2",
    "outputId": "476c3f65-da64-4744-d52f-556b14e2aae0"
   },
   "outputs": [],
   "source": [
    "# # Big Sur\n",
    "# alpha = 0.01\n",
    "# 1-(1-alpha)**22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyoming\n",
    "alpha = 0.01\n",
    "1-(1-alpha)**32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8_wvF6rby08"
   },
   "source": [
    "Actually things are a bit worse. The bitemporal tests are manifestly not independent because consecutive tests have one image in common. The best one can say in this situation is\n",
    "\n",
    "$$\n",
    "\\alpha_T \\le (k-1)\\alpha, \\tag{3.2}\n",
    "$$\n",
    "\n",
    "or $\\alpha_T \\le 25\\%$ for $k=26$ and $\\alpha=0.01$ . If we wish to set a false positive rate of at most, say, 1% for the entire series, then each bitemporal test must have a significance level of $\\alpha=0.0004$ and a correspondingly large false negative rate $\\beta$. In other words  many significant changes may be missed.\n",
    "\n",
    "How to proceed? Perhaps by being a bit less ambitious at first and asking the simpler question: _Were there any changes at all over the interval?_ If the answer is affirmative, we can worry about how many there were and when they occurred later. Let's formulate this question as ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGqBb29T2bWW"
   },
   "source": [
    "### An omnibus test for change\n",
    "\n",
    "We'll start again with the easier  single polarization case. For the series of _VV_ intensity images acquired at times $t_1, t_2,\\dots t_k$, our null hypothesis is that, at a given pixel position,  there has been no change in the signal strengths $a_i=\\langle|S^{a_i}_{vv}|^2\\rangle$ over the entire period, i.e.,\n",
    "\n",
    "$$\n",
    "H_0:\\quad a_1 = a_2 = \\dots = a_k = a.\n",
    "$$\n",
    "\n",
    "The alternative hypothesis is that there was at least one change (and possibly many) over the interval. For the more mathematically inclined this can be written succinctly as\n",
    "\n",
    "$$\n",
    "H_1:\\quad \\exists\\ i,j :\\ a_i \\ne a_j,\n",
    "$$\n",
    "\n",
    "which says: there exist indices $i, j$ for which $a_i$ is not equal to $a_j$.\n",
    "\n",
    "Again, the likelihood functions are products of gamma distributions:\n",
    "\n",
    "$$\n",
    "L_1(a_1,\\dots,a_k) =\\prod_{i=1}^k p(s_i\\mid a_i) = {1\\over\\Gamma(m)^k}\\left[\\prod_i{a_i\\over m}\\right]^{-m}\\left[\\prod_i s_i\\right]^{m-1}\\exp(-m\\sum_i{s_i\\over a_i}) \\tag{3.3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_0(a)  = \\prod_{i=1}^k p(s_i\\mid a) = {1\\over\\Gamma(m)^k} \\left[{a\\over m}\\right]^{-mk}\\left[\\prod_i s_i\\right]^{m-1}\\exp(-{m\\over a}\\sum_i s_i) \\tag{3.4}\n",
    "$$\n",
    "\n",
    "and $L_1$ is maximized for $\\hat a_i = s_i,\\ i=1\\dots k,$ while $L_0$ is maximized for $\\hat a = {1\\over k}\\sum_i s_i$. So with a bit of simple algebra our likelihood ratio test statistic is\n",
    "\n",
    "$$\n",
    "Q_k = {L_0(\\hat a)\\over L_1(\\hat a_1,\\dots,\\hat a_k)} = \\left[k^k{\\prod_i s_i\\over (\\sum_i s_i)^k}\\right]^m \\tag{3.5}\n",
    "$$\n",
    "\n",
    "and is called an _omnibus test statistic_. Note that, for $k=2$, we get the bitemporal LRT given by  [Eq. (2.10)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2#the_likelihood_ratio_test).\n",
    "\n",
    "We can't expect to find an analytical expression for the probability distribution of this LRT statistic, so we will again invoke Wilks' Theorem and work with\n",
    "\n",
    "$$\n",
    "-2 \\log{Q_k} = \\big[k\\log{k}+\\sum_i\\log{s_i}-k\\log{\\sum_i s_i}\\big](-2m) \\tag{3.6}\n",
    "$$\n",
    "\n",
    "According to Wilks, it should be approximately chi square distributed with $k-1$ degrees of freedom under $H_0$. (Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmEjLX1tGs1K"
   },
   "source": [
    "The input cell below evaluates the test statistic Eq. (3.6) for a list of single polarization images. We prefer from now on to use as default the equivalent number of looks 4.4 that we discussed at the end of [Part 1](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#equivalent_number_of_looks) rather than the actual number of looks $m=5$, in the hope of getting a better agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1z2EGvCDD1ec"
   },
   "outputs": [],
   "source": [
    "def omnibus(im_list, m = 4.4):\n",
    "    \"\"\"Calculates the omnibus test statistic, monovariate case.\"\"\"\n",
    "    def log(current):\n",
    "        return ee.Image(current).log()\n",
    "\n",
    "    im_list = ee.List(im_list)\n",
    "    k = im_list.length()\n",
    "    klogk = k.multiply(k.log())\n",
    "    klogk = ee.Image.constant(klogk)\n",
    "    sumlogs = ee.ImageCollection(im_list.map(log)).reduce(ee.Reducer.sum())\n",
    "    logsum = ee.ImageCollection(im_list).reduce(ee.Reducer.sum()).log()\n",
    "    return klogk.add(sumlogs).subtract(logsum.multiply(k)).multiply(-2*m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJmqoSgrJZDn"
   },
   "source": [
    "Let's see if this test statistic does indeed follow the chi square distribution. First we define a small polygon _aoi\\_sub_ over the Thorne Moors (on the eastern side of the AOI) for which we hope there are few significant changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LZnZc2AIne2"
   },
   "source": [
    "## Here is a comparison for pixels in _aoi\\_sub_ with the chi square distribution with $k-1$ degrees of freedom. We choose the first 12 images in the series ($k=10$) for Big Sur, and 16 in Wyoming because we expect fewer changes before the landslide than over the complete sequence $k=24$, which extends past the landslide event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "his4vdFXt8l2",
    "outputId": "5461307c-b8df-428a-bd39-f6575e32f676"
   },
   "outputs": [],
   "source": [
    "# # Big Sur\n",
    "# k = 12\n",
    "# hist = (omnibus(vv_list.slice(0,k))\n",
    "#         .reduceRegion(ee.Reducer.fixedHistogram(0, 40, 200), geometry=aoi, scale=10)\n",
    "#         .get('constant')\n",
    "#         .getInfo())\n",
    "\n",
    "# a = np.array(hist)\n",
    "# x = a[:,0]\n",
    "# y = a[:,1]/np.sum(a[:,1])\n",
    "# plt.plot(x, y, '.', label='data')\n",
    "# plt.plot(x, chi2.pdf(x, k-1)/5, '-r', label='chi square')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyoming\n",
    "k = 15\n",
    "hist = (omnibus(vv_list.slice(0,k))\n",
    "        .reduceRegion(ee.Reducer.fixedHistogram(0, 40, 200), geometry=aoi, scale=10)\n",
    "        .get('constant')\n",
    "        .getInfo())\n",
    "\n",
    "a = np.array(hist)\n",
    "x = a[:,0]\n",
    "y = a[:,1]/np.sum(a[:,1])\n",
    "plt.plot(x, y, '.', label='data')\n",
    "plt.plot(x, chi2.pdf(x, k-1)/5, '-r', label='chi square')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgZuh94UsEZf"
   },
   "source": [
    "It appears that Wilks' Theorem is again a fairly good approximation. So why not generate a change map for the full series? The good news is that we now have the overall false positive probability $\\alpha$ under control. Here we set it to $\\alpha=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CL4N3GednWs9",
    "outputId": "95c81f36-7c3e-45d2-cfb9-080b8d8e2985"
   },
   "outputs": [],
   "source": [
    "# # The Big Sur change map for alpha = 0.01.\n",
    "# k = len(timestamplist); alpha = 0.01\n",
    "# p_value = ee.Image.constant(1).subtract(chi2cdf(omnibus(vv_list), k-1))\n",
    "# c_map = p_value.multiply(0).where(p_value.lt(alpha), 1)\n",
    "# # Make the no-change pixels transparent.\n",
    "# c_map = c_map.updateMask(c_map.gt(0))\n",
    "# # Overlay onto the folium map.\n",
    "# location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "# mp = folium.Map(location=location, zoom_start=16)\n",
    "# folium.Marker(\n",
    "#     [35.865628, -121.4323838], popup=\"<i>Big Sur Landslide</i>\"\n",
    "# ).add_to(mp)\n",
    "# mp.add_ee_layer(c_map, {'min': 0,'max': 1, 'palette': ['black', 'red']}, 'change map')\n",
    "# mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Wyoming change map for alpha = 0.01.\n",
    "k = len(timestamplist); alpha = 0.01\n",
    "p_value = ee.Image.constant(1).subtract(chi2cdf(omnibus(vv_list), k-1))\n",
    "c_map = p_value.multiply(0).where(p_value.lt(alpha), 1)\n",
    "# Make the no-change pixels transparent.\n",
    "c_map = c_map.updateMask(c_map.gt(0))\n",
    "# Overlay onto the folium map.\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "mp = folium.Map(location=location, zoom_start=11)\n",
    "folium.Marker(\n",
    "    [43.51364004, -108.1852766], popup=\"<i>Wyoming Landslide</i>\"\n",
    ").add_to(mp)\n",
    "mp.add_ee_layer(c_map, {'min': 0,'max': 1, 'palette': ['black', 'red']}, 'change map')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OFU7fgvtzCm"
   },
   "source": [
    "So plenty of changes, but hard to interpret considering the time span. Although we can see _where_ changes took place, we know neither _when_ they occurred nor their _multiplicity_. Also there is a matter that we have glossed over up until now, and that is ...\n",
    "\n",
    "### A question of scale\n",
    "\n",
    "The number of looks plays an important role in all of the formulae that we have discussed so far, and for the Sentinel-1 ground range detected imagery we first used $m=5$ and now the ENL $=4.4$.  When we display a change map interactively, the  [zoom factor determines the image pyramid level](https://developers.google.com/earth-engine/guides/scale) at which the GEE servers perform the required calculations and pass the result to the folium map client. If the calculations are not at the nominal scale of 10m then the number of looks is effectively larger than the ENL due to the averaging involved in constructing higher pyramid levels. The effect can be seen in the  output cell above: the number of change pixels seems to decrease when we zoom out. There is no problem when we export our results to GEE assets, to Google Drive or to Cloud storage, since we can simply choose the correct nominal scale for export.\n",
    "\n",
    "In order to see the changes correctly at all zoom levels, we can force GEE to work at the nominal scale by reprojecting before displaying on the map ([use with caution](https://developers.google.com/earth-engine/guides/projections#reprojecting)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "shwzGq2tWMva",
    "outputId": "66aae4c7-e6f8-48c8-9d46-e557cc780856"
   },
   "outputs": [],
   "source": [
    "# # Big Sur \n",
    "# c_map_10m = c_map.reproject(c_map.projection().crs(), scale=10)\n",
    "# mp = folium.Map(location=location, zoom_start=16)\n",
    "# mp.add_ee_layer(c_map, {'min': 0, 'max': 1, 'palette': ['black', 'red']}, 'Change map')\n",
    "# mp.add_ee_layer(c_map_10m, {'min': 0, 'max': 1, 'palette': ['black', 'blue']}, 'Change map (10m)')\n",
    "# folium.Marker(\n",
    "#     [35.865628, -121.4323838], popup=\"<i>Big Sur Landslide</i>\"\n",
    "# ).add_to(mp)\n",
    "# mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyoming \n",
    "c_map_10m = c_map.reproject(c_map.projection().crs(), scale=10)\n",
    "mp = folium.Map(location=location, zoom_start=11)\n",
    "mp.add_ee_layer(c_map, {'min': 0, 'max': 1, 'palette': ['black', 'red']}, 'Change map')\n",
    "mp.add_ee_layer(c_map_10m, {'min': 0, 'max': 1, 'palette': ['black', 'blue']}, 'Change map (10m)')\n",
    "folium.Marker(\n",
    "    [43.51364004, -108.1852766], popup=\"<i>Big Sur Landslide</i>\"\n",
    ").add_to(mp)\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTkiTAIWLxpU"
   },
   "source": [
    "You will notice in the output cell above that the calculation at nominal scale (the blue pixels) now takes considerably longer to complete. Also some red pixels are not completely covered by blue ones. Those changes are a spurious result of the falsified number of looks. Nevertheless for quick previewing purposes we might prefer to do without the reprojection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9mlvH7oUtJe"
   },
   "source": [
    "### A sequential omnibus test\n",
    "\n",
    "Recalling the last remark at the end of [Part 2](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2#oh_and_one_more_thing_), let's now guess the omnibus LRT for the dual polarization case. From Eq. (3.5), replacing $s_i \\to|c_i|$,  $\\ \\sum s_i \\to |\\sum c_i|\\ $ and $k^k \\to k^{2k}$, we get\n",
    "\n",
    "$$\n",
    "Q_k =  \\left[k^{2k}{\\prod_i |c_i|\\over |\\sum_i c_i|^k}\\right]^m. \\tag{3.7}\n",
    "$$\n",
    "\n",
    "This is in fact a special case of a more general omnibus test statistic\n",
    "\n",
    "$$\n",
    "Q_k =  \\left[k^{pk}{\\prod_i |c_i|\\over |\\sum_i c_i|^k}\\right]^m\n",
    "$$\n",
    "\n",
    "which holds for $p\\times p$ polarimetric covariance matrix images, for example for the full dual pol matrix   [Eq. (1.5)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-1#single_look_complex_slc_sar_measurements) or for full $3\\times 3$ quad pol matrices ($p=3$), but also for diagonal $2\\times 2$ and $3\\times 3$ matrices.\n",
    "\n",
    "Which brings us to the **heart of this Tutorial**. We will now decompose Eq. (3.7) into a product of independent likelihood ratio tests which will enable us to determine when changes occurred at each pixel location. Then we'll code a complete multitemporal change detection algorithm on the GEE Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tePZxpVI0Tkg"
   },
   "source": [
    "#### Single polarization\n",
    "\n",
    "Rather than make a formal derivation, we will illustrate the decomposition on a series of $k=5$ single polarization (VV) measurements. The omnibus test Eq. (3.5) for any change over the series from $t_1$ to $t_5$ is\n",
    "\n",
    "$$\n",
    "Q_5 = \\left[ 5^5 {s_1s_2s_3s_4s_5\\over (s_1+s_2+s_3+s_4+s_5)^5}\\right]^m.\n",
    "$$\n",
    "\n",
    "If we accept the null hypothesis $a_1=a_2=a_3=a_4=a_5$ we're done and can move on to the next pixel (figuratively of course, since this stuff is all done in parallel). But suppose we have rejected the null hypothesis, i.e., there was a least one significant change. In order to find it (or them), we begin by testing the first of the four intervals. That's just the bitemporal test from Part 2, but let's call it $R_2$ rather than $Q_2$,\n",
    "\n",
    "$$\n",
    "R_2 = \\left[ 2^2 {s_1s_2\\over (s_1+s_2)^2}\\right]^m.\n",
    "$$\n",
    "\n",
    "Suppose we conclude no change, that is, $a_1=a_2$. Now we don't do just another bitemporal test on the second interval. Instead we test the hypothesis\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0:\\ & a_1=a_2= a_3\\ (=a)\\cr\n",
    "{\\rm against}\\quad H_1:\\  &a_1=a_2\\ (=a) \\ne a_3.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So the alternative hypothesis is: _There was no change in the first interval **and** there was a change in the second interval_. The LRT is easy to derive, but let's go through it anyway.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "        {\\rm From\\ Eq.}\\ (3.4):\\  &L_0(a)  = {1\\over\\Gamma(m)^3} \\left[{a\\over m}\\right]^{-3m}\\left[s_1s_2s_3\\right]^{m-1}\\exp(-{m\\over a}(s_1+s_2+s_3)  \\cr\n",
    "        &\\hat a = {1\\over 3}(s_1+s_2+s_3) \\cr\n",
    "=>\\           &L_0(\\hat a) = {1\\over\\Gamma(m)^3} \\left[{s_1+s_2+s_3\\over 3m}\\right]^{-3m}\\left[s_1s_2s_3\\right]^{m-1} \\exp(-3m) \\cr\n",
    "{\\rm From\\ Eq.}\\ (3.3):\\ &L_1(a_1,a_2,a_3) = {1\\over\\Gamma(m)^3}\\left[a_1a_2a_3\\over m\\right]^{-m}[s_1s_2s_3]^{m-1}\\exp(-m(s_1/a_1+s_2/a_2+s_3/a_3)\\cr\n",
    "&\\hat a_1 = \\hat a_2 = {1\\over 2}(s_1+s_2),\\quad \\hat a_3 = s_3 \\cr\n",
    "=>\\ &L_1(\\hat a_1,\\hat a_2, \\hat a_3) = {1\\over\\Gamma(m)^3}\\left[(s_1+s_2)^2s_3\\over 2^2m \\right]^{-m}[s_1s_2s_3]^{m-1}\\exp(-3m)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "And, taking the ratio $L_0/L_1$of the maximum likelihoods,\n",
    "\n",
    "$$\n",
    "R_3 = \\left[{3^3\\over 2^2}{(s_1+s_2)^2s_3\\over (s_1+s_2+s_3)^3}\\right]^m.\n",
    "$$\n",
    "\n",
    "Not too hard to guess that, if we accept $H_0$ again, we go on to test\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H_0:\\ a_1=a_2=a_3=a_4\\ (=a)\\cr\n",
    "{\\rm against}\\quad H_1:\\ a_1=a_2=a_3\\ (=a) \\ne a_4.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with LRT statistic\n",
    "\n",
    "$$\n",
    "R_4 = \\left[{4^4\\over 3^3}{(s_1+s_2+s_3)^3s_4\\over (s_1+s_2+s_3+s_4)^4}\\right]^m,\n",
    "$$\n",
    "\n",
    "and so on to $R_5$ and the end of the time series.\n",
    "\n",
    "Now for the cool part (try it out yourself):\n",
    "\n",
    "$$\n",
    "R_2\\times R_3\\times R_4 \\times R_5 = Q_5.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twkipaPuT1qP"
   },
   "source": [
    "So, generalizing to a series of length $k$:\n",
    "\n",
    "**The omnibus test statistic $Q_k$ may be factored into the product of  LRT's $R_j$ which test for homogeneity in the measured reflectance signal up to and including time $t_j$, assuming homogeneity up to time $t_{j-1}$:**\n",
    "\n",
    "$$\n",
    "Q_k = \\prod_{j=2}^k R_j, \\quad R_j = \\left[{j^j\\over (j-1)^{j-1}}{(s_1+\\dots +s_{j-1})^{j-1}s_j\\over (s_1+\\dots +s_j)^j}\\right]^m,\\quad j = 2\\dots k.  \\tag{3.8}\n",
    "$$\n",
    "\n",
    "Moreover the test statistics $R_j$ are stochastically independent under $H_0$.\n",
    "This can be shown analytically, see [Conradsen et al. (2016)](https://ieeexplore.ieee.org/document/7398022) or P. 405 in my [textbook](https://www.taylorfrancis.com/books/9780429464348), but we'll show it here empirically by sampling the test statistics $R_j$ in the region _aoi\\_sub_ and examining the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GSB1pSnNee6",
    "outputId": "095b7cf8-a3cd-4b7c-8947-835686cd29b2"
   },
   "outputs": [],
   "source": [
    "def sample_vv_imgs(j):\n",
    "    \"\"\"Samples the test statistics Rj in the region aoi.\"\"\"\n",
    "    j = ee.Number(j)\n",
    "    # Get the factors in the expression for Rj.\n",
    "    sj = vv_list.get(j.subtract(1))\n",
    "    jfact = j.pow(j).divide(j.subtract(1).pow(j.subtract(1)))\n",
    "    sumj = ee.ImageCollection(vv_list.slice(0, j)).reduce(ee.Reducer.sum())\n",
    "    sumjm1 = ee.ImageCollection(vv_list.slice(0, j.subtract(1))).reduce(ee.Reducer.sum())\n",
    "    # Put them together.\n",
    "    Rj = sumjm1.pow(j.subtract(1)).multiply(sj).multiply(jfact).divide(sumj.pow(j)).pow(5)\n",
    "    # Sample Rj.\n",
    "    sample = (Rj.sample(region=aoi, scale=10, numPixels=1000, seed=123)\n",
    "              .aggregate_array('VV_sum'))\n",
    "    return sample\n",
    "\n",
    "# Sample the first few list indices.\n",
    "samples = ee.List.sequence(2, 5).map(sample_vv_imgs)\n",
    "\n",
    "# Calculate and display the correlation matrix.\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# print(np.corrcoef(samples.getInfo()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1wKex1oFrqU"
   },
   "source": [
    "The off-diagonal elements are mostly small. The not-so-small values can be attributed to sampling error or to the presence of some change pixels in the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFzoRyIOU5Rx"
   },
   "source": [
    "#### Dual polarization and an algorithm\n",
    "\n",
    "With our substitution trick, we can now write down the sequential test for the dual polarization (bivariate) image time series. From Eq. (3.8) we get\n",
    "\n",
    "$$\n",
    "Q_k = \\prod_{j=2}^k R_j , \\quad R_j = \\left[{j^{2j}\\over (j-1)^{2(j-1)}}{|c_1+\\dots +c_{j-1}|^{j-1}|c_j|\\over |c_1+\\dots +c_j|^j}\\right]^m,\\quad j = 2\\dots k. \\tag{3.9}\n",
    "$$\n",
    "\n",
    "And of course we have again to use Wilks' Theorem to get the _P_ values, so we work with\n",
    "\n",
    "$$\n",
    "-2\\log{R_j} = -2m\\Big[2(j\\log{j}-(j-1)\\log(j-1)+(j-1)\\log\\Big|\\sum_{i=1}^{j-1}c_i \\Big|+\\log|c_j|-j\\log\\Big|\\sum_{i=1}^j c_i\\Big|\\ \\Big] \\tag{3.10a}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "-2\\log Q_k = \\sum_{j=2}^k -2\\log R_j. \\tag{3.10b}\n",
    "$$\n",
    "\n",
    "The statistic $-2\\log R_j$ is approximately chi square distributed with two degrees of freedom. Similarly $-2\\log Q_k$ is approximately chi square distributed with $2(k-1)$ degrees of freedom. Readers should satisfy themselves that these numbers are indeed the correct, taking into account that each measurement $c_i$ has two free parameters $|S^a_{vv}|^2$ and $|S^b_{vh}|^2$, see [Eq. (2.13)](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2#bivariate_change_detection).\n",
    "\n",
    "Now for the algorithm:\n",
    "\n",
    "**The sequential omnibus change detection algorithm**\n",
    "\n",
    "With a time series of $k$ SAR images $(c_1,c_2,\\dots,c_k)$,\n",
    "\n",
    "1. Set $\\ell = k$.\n",
    "2. Set $s = (c_{k-\\ell+1}, \\dots c_k)$.\n",
    "3. Perform the omnibus test $Q_\\ell$ for any changes change over $s$.\n",
    "4. If no significant changes are found, stop.\n",
    "5. Successively test series $s$ with $R_2, R_3, \\dots$ until the first significant change is met for $R_j$.\n",
    "6. Set $\\ell = k-j+1$ and go to 2.\n",
    "\n",
    "|Table 3.1 |       |       |       |       |       |        |\n",
    "|----------|-------|-------|-------|-------|-------|--------|\n",
    "|  $\\ell$  | $c_1$ | $c_2$ | $c_3$ | $c_4$ | $c_5$ |        |\n",
    "| 5        |       | $R^5_2$ | $R^5_3$ | $R^5_4$ | $R^5_5$ | $Q_5$  |\n",
    "| 4        |       |       | $R^4_2$ | $R^4_3$ | $R^4_4$ | $Q_4$  |\n",
    "| 3        |       |       |       | $R^3_2$ | $R^3_3$ | $Q_3$  |\n",
    "| 2        |       |       |       |       | $R^2_2$ | $Q_2$  |\n",
    "\n",
    "\n",
    "Thus if a change is found, the series is truncated up to the point of change and the testing procedure is repeated for the rest of the series. Take for example a series of $k=5$ images. (See Table 3.1 where, to avoid ambiguity, we add superscript $\\ell$ to each $R_j$ test). Suppose there is one change in the second interval only. Then the test sequence is (the asterisk means $H_0$ is rejected)\n",
    "\n",
    "$$\n",
    "Q^*_5 \\to R^5_2 \\to R^{5*}_3 \\to Q_3.\n",
    "$$\n",
    "\n",
    "If there are changes in the second and last intervals,\n",
    "\n",
    "$$\n",
    "Q^*_5 \\to R^5_2 \\to R^{5*}_3 \\to Q^*_3 \\to R^3_2 \\to R^{3*}_3,\n",
    "$$\n",
    "\n",
    "and if there are significant changes in all four intervals,\n",
    "\n",
    "$$\n",
    "Q^*_5 \\to R^{5*}_2 \\to Q^*_4 \\to R^{4*}_2 \\to Q^*_3 \\to R^{3*}_2 \\to Q^*_2.\n",
    "$$\n",
    "\n",
    "The approach taken in the coding of this algorithm is to pre-calculate  _P_ values for all of the $Q_\\ell / R_j$ tests and then, in a second pass, to filter them to determine the points of change.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0fpZbEfthH3"
   },
   "source": [
    "#### Pre-calculating the _P_ value array\n",
    "\n",
    "The following code cell performs map operations on the indices $\\ell$ and $j$, returning an array of _P_ values for all possible LRT statistics. For example again for $k=5$, the code calculates the _P_ values for each $R_j$ entry in Table 3.1 as a list of lists. Before calculating each row, the time series $c_1, c_2,c_3,c_4, c_5$ is sliced from $k-\\ell+1$ to $k$. The last entry in each row is simply the product of the other entries,  $Q_\\ell =\\prod_{j=2}^\\ell R_j.$\n",
    "\n",
    "The program actually operates on the logarithms of the test statistics, Equations (3.10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DDfm-FxtylT"
   },
   "outputs": [],
   "source": [
    "def log_det_sum(im_list, j):\n",
    "    \"\"\"Returns log of determinant of the sum of the first j images in im_list.\"\"\"\n",
    "    im_ist = ee.List(im_list)\n",
    "    sumj = ee.ImageCollection(im_list.slice(0, j)).reduce(ee.Reducer.sum())\n",
    "    return ee.Image(det(sumj)).log()\n",
    "\n",
    "def log_det(im_list, j):\n",
    "    \"\"\"Returns log of the determinant of the jth image in im_list.\"\"\"\n",
    "    im = ee.Image(ee.List(im_list).get(j.subtract(1)))\n",
    "    return ee.Image(det(im)).log()\n",
    "\n",
    "def pval(im_list, j, m=4.4):\n",
    "    \"\"\"Calculates -2logRj for im_list and returns P value and -2logRj.\"\"\"\n",
    "    im_list = ee.List(im_list)\n",
    "    j = ee.Number(j)\n",
    "    m2logRj = (log_det_sum(im_list, j.subtract(1))\n",
    "               .multiply(j.subtract(1))\n",
    "               .add(log_det(im_list, j))\n",
    "               .add(ee.Number(2).multiply(j).multiply(j.log()))\n",
    "               .subtract(ee.Number(2).multiply(j.subtract(1))\n",
    "               .multiply(j.subtract(1).log()))\n",
    "               .subtract(log_det_sum(im_list,j).multiply(j))\n",
    "               .multiply(-2).multiply(m))\n",
    "    pv = ee.Image.constant(1).subtract(chi2cdf(m2logRj, 2))\n",
    "    return (pv, m2logRj)\n",
    "\n",
    "def p_values(im_list):\n",
    "    \"\"\"Pre-calculates the P-value array for a list of images.\"\"\"\n",
    "    im_list = ee.List(im_list)\n",
    "    k = im_list.length()\n",
    "\n",
    "    def ells_map(ell):\n",
    "        \"\"\"Arranges calculation of pval for combinations of k and j.\"\"\"\n",
    "        ell = ee.Number(ell)\n",
    "        # Slice the series from k-l+1 to k (image indices start from 0).\n",
    "        im_list_ell = im_list.slice(k.subtract(ell), k)\n",
    "\n",
    "        def js_map(j):\n",
    "            \"\"\"Applies pval calculation for combinations of k and j.\"\"\"\n",
    "            j = ee.Number(j)\n",
    "            pv1, m2logRj1 = pval(im_list_ell, j)\n",
    "            return ee.Feature(None, {'pv': pv1, 'm2logRj': m2logRj1})\n",
    "\n",
    "        # Map over j=2,3,...,l.\n",
    "        js = ee.List.sequence(2, ell)\n",
    "        pv_m2logRj = ee.FeatureCollection(js.map(js_map))\n",
    "\n",
    "        # Calculate m2logQl from collection of m2logRj images.\n",
    "        m2logQl = ee.ImageCollection(pv_m2logRj.aggregate_array('m2logRj')).sum()\n",
    "        pvQl = ee.Image.constant(1).subtract(chi2cdf(m2logQl, ell.subtract(1).multiply(2)))\n",
    "        pvs = ee.List(pv_m2logRj.aggregate_array('pv')).add(pvQl)\n",
    "        return pvs\n",
    "\n",
    "    # Map over l = k to 2.\n",
    "    ells = ee.List.sequence(k, 2, -1)\n",
    "    pv_arr = ells.map(ells_map)\n",
    "\n",
    "    # Return the P value array ell = k,...,2, j = 2,...,l.\n",
    "    return pv_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx3XiJAEIpu7"
   },
   "source": [
    "#### Filtering the _P_ values\n",
    "\n",
    "|Table 3.2 |       |       |       |       |       |        |\n",
    "|----------|-------|-------|-------|-------|-------|--------|\n",
    "|$i\\ $ / $j$|      |     1 |     2 |     3 |     4 |        |\n",
    "| 1        |       | $P_2$ | $P_3$ | $P_4$ | $P_5$ | $P_{Q5}$  |\n",
    "| 2        |       |       | $P_2$ | $P_3$ | $P_4$ | $P_{Q4}$  |\n",
    "| 3        |       |       |       | $P_2$ | $P_3$ | $P_{Q3}$  |\n",
    "| 4        |       |       |       |       | $P_2$ | $P_{Q2}$  |\n",
    "\n",
    "The pre-calculated _P_ values in _pv\\_arr_ (shown schematically in Table 3.2 for $k=5$) are then scanned in nested iterations over indices $i$ and $j$ to determine the following thematic change maps:\n",
    "\n",
    "- cmap: the interval of the most recent change, one band, byte values $\\in [0,k-1]$,\n",
    "- smap: the interval of the first change, one band, byte values $\\in [0,k-1]$,\n",
    "- fmap: the number of changes, one band, byte values $\\in [0,k-1]$,\n",
    "- bmap: the changes in each interval, $\\ k-1$ bands, byte values $\\in [0,1]$).\n",
    "\n",
    "A boolean variable _median_ is included in the code. Its purpose is to reduce the salt-and-pepper effect in no-change regions, which is at least partly a consequence of the uniform distribution of the _P_ values under $H_0$ (see the section [A note on P values](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2#a_note_on_p_values) in Part 2). If _median_ is _True_, the _P_ values for each $Q_\\ell$ statistic are passed through a $5\\times 5$ median filter before being compared with the significance threshold. This is not statistically kosher but probably justifiable if one is only interested in large homogeneous changes, for example flood inundations or deforestation.\n",
    "\n",
    "Here is the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1KBQwPWLYEI"
   },
   "outputs": [],
   "source": [
    "def filter_j(current, prev):\n",
    "    \"\"\"Calculates change maps; iterates over j indices of pv_arr.\"\"\"\n",
    "    pv = ee.Image(current)\n",
    "    prev = ee.Dictionary(prev)\n",
    "    pvQ = ee.Image(prev.get('pvQ'))\n",
    "    i = ee.Number(prev.get('i'))\n",
    "    cmap = ee.Image(prev.get('cmap'))\n",
    "    smap = ee.Image(prev.get('smap'))\n",
    "    fmap = ee.Image(prev.get('fmap'))\n",
    "    bmap = ee.Image(prev.get('bmap'))\n",
    "    alpha = ee.Image(prev.get('alpha'))\n",
    "    j = ee.Number(prev.get('j'))\n",
    "    cmapj = cmap.multiply(0).add(i.add(j).subtract(1))\n",
    "    # Check      Rj?            Ql?                  Row i?\n",
    "    tst = pv.lt(alpha).And(pvQ.lt(alpha)).And(cmap.eq(i.subtract(1)))\n",
    "    # Then update cmap...\n",
    "    cmap = cmap.where(tst, cmapj)\n",
    "    # ...and fmap...\n",
    "    fmap = fmap.where(tst, fmap.add(1))\n",
    "    # ...and smap only if in first row.\n",
    "    smap = ee.Algorithms.If(i.eq(1), smap.where(tst, cmapj), smap)\n",
    "    # Create bmap band and add it to bmap image.\n",
    "    idx = i.add(j).subtract(2)\n",
    "    tmp = bmap.select(idx)\n",
    "    bname = bmap.bandNames().get(idx)\n",
    "    tmp = tmp.where(tst, 1)\n",
    "    tmp = tmp.rename([bname])\n",
    "    bmap = bmap.addBands(tmp, [bname], True)\n",
    "    return ee.Dictionary({'i': i, 'j': j.add(1), 'alpha': alpha, 'pvQ': pvQ,\n",
    "                          'cmap': cmap, 'smap': smap, 'fmap': fmap, 'bmap':bmap})\n",
    "\n",
    "def filter_i(current, prev):\n",
    "    \"\"\"Arranges calculation of change maps; iterates over row-indices of pv_arr.\"\"\"\n",
    "    current = ee.List(current)\n",
    "    pvs = current.slice(0, -1 )\n",
    "    pvQ = ee.Image(current.get(-1))\n",
    "    prev = ee.Dictionary(prev)\n",
    "    i = ee.Number(prev.get('i'))\n",
    "    alpha = ee.Image(prev.get('alpha'))\n",
    "    median = prev.get('median')\n",
    "    # Filter Ql p value if desired.\n",
    "    pvQ = ee.Algorithms.If(median, pvQ.focalMedian(2.5), pvQ)\n",
    "    cmap = prev.get('cmap')\n",
    "    smap = prev.get('smap')\n",
    "    fmap = prev.get('fmap')\n",
    "    bmap = prev.get('bmap')\n",
    "    first = ee.Dictionary({'i': i, 'j': 1, 'alpha': alpha ,'pvQ': pvQ,\n",
    "                           'cmap': cmap, 'smap': smap, 'fmap': fmap, 'bmap': bmap})\n",
    "    result = ee.Dictionary(ee.List(pvs).iterate(filter_j, first))\n",
    "    return ee.Dictionary({'i': i.add(1), 'alpha': alpha, 'median': median,\n",
    "                          'cmap': result.get('cmap'), 'smap': result.get('smap'),\n",
    "                          'fmap': result.get('fmap'), 'bmap': result.get('bmap')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjTNE7fC5YGl"
   },
   "source": [
    "The following function ties the two steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iuzc7fZ-0s1P"
   },
   "outputs": [],
   "source": [
    "def change_maps(im_list, median=False, alpha=0.01):\n",
    "    \"\"\"Calculates thematic change maps.\"\"\"\n",
    "    k = im_list.length()\n",
    "    # Pre-calculate the P value array.\n",
    "    pv_arr = ee.List(p_values(im_list))\n",
    "    # Filter P values for change maps.\n",
    "    cmap = ee.Image(im_list.get(0)).select(0).multiply(0)\n",
    "    bmap = ee.Image.constant(ee.List.repeat(0, k.subtract(1))).add(cmap)\n",
    "    alpha = ee.Image.constant(alpha)\n",
    "    first = ee.Dictionary({'i': 1, 'alpha': alpha, 'median': median,\n",
    "                           'cmap': cmap, 'smap': cmap, 'fmap': cmap, 'bmap': bmap})\n",
    "    return ee.Dictionary(pv_arr.iterate(filter_i, first))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz2FqH45CVpe"
   },
   "source": [
    "And now we run the algorithm and display the color-coded change maps: _cmap_, _smap_ (blue early, red late) and _fmap_ (blue few, red many):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T4dvtMlna_8R",
    "outputId": "d8f19183-72b8-403f-d4d5-3372bba4cc7b"
   },
   "outputs": [],
   "source": [
    "result = change_maps(im_list, median=True, alpha=0.05)\n",
    "\n",
    "# Extract the change maps and display.\n",
    "cmap = ee.Image(result.get('cmap'))\n",
    "smap = ee.Image(result.get('smap'))\n",
    "fmap = ee.Image(result.get('fmap'))\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "palette = ['black', 'blue', 'cyan', 'yellow', 'red']\n",
    "mp = folium.Map(location=location, zoom_start=16)\n",
    "folium.Marker(\n",
    "    [35.865628, -121.4323838], popup=\"<i>Big Sur Landslide</i>\"\n",
    ").add_to(mp)\n",
    "mp.add_ee_layer(cmap, {'min': 0, 'max': 25, 'palette': palette}, 'cmap')\n",
    "mp.add_ee_layer(smap, {'min': 0, 'max': 25, 'palette': palette}, 'smap')\n",
    "mp.add_ee_layer(fmap, {'min': 0, 'max': 25, 'palette': palette}, 'fmap')\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lq1VHnoI23L"
   },
   "source": [
    "#### Post-processing: The Loewner order\n",
    "\n",
    "The above change maps are still difficult to interpret. But what about _bmap_, the map of changes detected in each interval? Before we look at them it makes sense to include the direction of change, i.e., the [Loewner order](https://ieeexplore.ieee.org/document/8736751), see [Part 2](https://developers.google.com/earth-engine/tutorials/community/detecting-changes-in-sentinel-1-imagery-pt-2#change_direction_the_loewner_order). In the event of significant change at time $j$, we can simply determine the positive or negative definiteness (or indefiniteness) of the difference between consecutive covariance matrix pixels\n",
    "\n",
    "$$\n",
    "c_j-c_{j-1},\\quad j = 2,\\dots,k,\n",
    "$$\n",
    "\n",
    "to get the change direction. But we can do better. Instead of subtracting the value for the preceding image, $c_{j-1}$, we can subtract the average over all values up to and including time $j-1$ for which no change has been signalled. For example for $k=5$, suppose there are significant changes in the first and fourth (last) interval. Then to get their directions we examine the differences\n",
    "\n",
    "$$\n",
    "c_2-c_1\\quad{\\rm and}\\quad c_5 - (c_2+c_3+c_4)/3.\n",
    "$$\n",
    "\n",
    "The running averages can be conveniently determined with the so-called _provisional means algorithm_. The average $\\bar c_i$ of the first $i$ images is calculated recursively as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\bar c_i &= \\bar c_{i-1} + (c_i - \\bar c_{i-1})/i \\cr\n",
    "\\bar c_1 &= c_1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The function _dmap\\_iter_ below is iterated over the bands of _bmap_, replacing the values for changed pixels with\n",
    "\n",
    "- 1 for positive definite differences,\n",
    "- 2 for negative definite differences,\n",
    "- 3 for indefinite differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aG9UoPHH29tk"
   },
   "outputs": [],
   "source": [
    "def dmap_iter(current, prev):\n",
    "    \"\"\"Reclassifies values in directional change maps.\"\"\"\n",
    "    prev = ee.Dictionary(prev)\n",
    "    j = ee.Number(prev.get('j'))\n",
    "    image = ee.Image(current)\n",
    "    avimg = ee.Image(prev.get('avimg'))\n",
    "    diff = image.subtract(avimg)\n",
    "    # Get positive/negative definiteness.\n",
    "    posd = ee.Image(diff.select(0).gt(0).And(det(diff).gt(0)))\n",
    "    negd = ee.Image(diff.select(0).lt(0).And(det(diff).gt(0)))\n",
    "    bmap = ee.Image(prev.get('bmap'))\n",
    "    bmapj = bmap.select(j)\n",
    "    dmap = ee.Image.constant(ee.List.sequence(1, 3))\n",
    "    bmapj = bmapj.where(bmapj, dmap.select(2))\n",
    "    bmapj = bmapj.where(bmapj.And(posd), dmap.select(0))\n",
    "    bmapj = bmapj.where(bmapj.And(negd), dmap.select(1))\n",
    "    bmap = bmap.addBands(bmapj, overwrite=True)\n",
    "    # Update avimg with provisional means.\n",
    "    i = ee.Image(prev.get('i')).add(1)\n",
    "    avimg = avimg.add(image.subtract(avimg).divide(i))\n",
    "    # Reset avimg to current image and set i=1 if change occurred.\n",
    "    avimg = avimg.where(bmapj, image)\n",
    "    i = i.where(bmapj, 1)\n",
    "    return ee.Dictionary({'avimg': avimg, 'bmap': bmap, 'j': j.add(1), 'i': i})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7h1GuQ79-jH"
   },
   "source": [
    "We only have to modify the _change\\_maps_ function to include the change direction in the _bmap_ image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CF3-_d6M5JGf"
   },
   "outputs": [],
   "source": [
    "def change_maps(im_list, median=False, alpha=0.01):\n",
    "    \"\"\"Calculates thematic change maps.\"\"\"\n",
    "    k = im_list.length()\n",
    "    # Pre-calculate the P value array.\n",
    "    pv_arr = ee.List(p_values(im_list))\n",
    "    # Filter P values for change maps.\n",
    "    cmap = ee.Image(im_list.get(0)).select(0).multiply(0)\n",
    "    bmap = ee.Image.constant(ee.List.repeat(0,k.subtract(1))).add(cmap)\n",
    "    alpha = ee.Image.constant(alpha)\n",
    "    first = ee.Dictionary({'i': 1, 'alpha': alpha, 'median': median,\n",
    "                           'cmap': cmap, 'smap': cmap, 'fmap': cmap, 'bmap': bmap})\n",
    "    result = ee.Dictionary(pv_arr.iterate(filter_i, first))\n",
    "    # Post-process bmap for change direction.\n",
    "    bmap =  ee.Image(result.get('bmap'))\n",
    "    avimg = ee.Image(im_list.get(0))\n",
    "    j = ee.Number(0)\n",
    "    i = ee.Image.constant(1)\n",
    "    first = ee.Dictionary({'avimg': avimg, 'bmap': bmap, 'j': j, 'i': i})\n",
    "    dmap = ee.Dictionary(im_list.slice(1).iterate(dmap_iter, first)).get('bmap')\n",
    "    return ee.Dictionary(result.set('bmap', dmap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL3TPfxXdFaQ"
   },
   "source": [
    "Because of the long delays when the zoom level is changed, it is a lot more convenient to export the change maps to GEE Assets and then examine them, either here in Colab or in the Code Editor. This also means the maps will be shown at the correct scale, irrespective of the zoom level. Here I export all of the change maps as a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsSpbk74RViE"
   },
   "outputs": [],
   "source": [
    "# Run the algorithm with median filter and at 1% significance.\n",
    "result = ee.Dictionary(change_maps(im_list, median=True, alpha=0.01))\n",
    "# Extract the change maps and export to assets.\n",
    "cmap = ee.Image(result.get('cmap')).toDouble()\n",
    "smap = ee.Image(result.get('smap')).toDouble()\n",
    "fmap = ee.Image(result.get('fmap')).toDouble()\n",
    "bmap = ee.Image(result.get('bmap')).toDouble()\n",
    "cmaps = ee.Image.cat(cmap, smap, fmap, bmap).rename(['cmap', 'smap', 'fmap']+timestamplist[1:])\n",
    "\n",
    "# EDIT THE ASSET PATH TO POINT TO YOUR ACCOUNT.\n",
    "# assetId = 'users/leah.manak/cmaps'\n",
    "\n",
    "# assexport = ee.batch.Export.image.toAsset(cmaps,\n",
    "#                                           description='assetExportTask',\n",
    "#                                           assetId=assetId, scale=10, maxPixels=1e9)\n",
    "\n",
    "# UNCOMMENT THIS TO EXPORT THE MAP TO YOUR ACCOUNT.\n",
    "#assexport.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eBeTB6sQMbd"
   },
   "source": [
    "The asset  _cmaps_ is shared so we can all access it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qseBmVOH7xPg",
    "outputId": "b99a43b1-5fd4-40bf-bb8e-ebb794d455d1"
   },
   "outputs": [],
   "source": [
    "# cmaps = ee.Image('projects/leah.manak/ee-landslide-project/detecting-changes-in-sentinel-1-imagery-pt-3/cmaps')\n",
    "cmaps = cmaps.updateMask(cmaps.gt(0))\n",
    "\n",
    "location = aoi.centroid().coordinates().getInfo()[::-1]\n",
    "palette = ['black', 'red', 'cyan', 'yellow', 'grey']\n",
    "mp = folium.Map(location=location, zoom_start=15)\n",
    "slide_img = 'T20170606'\n",
    "cmaps.select(slide_img)\n",
    "mp.add_ee_layer(\n",
    "    cmaps.select(slide_img),\n",
    "    {'min': 0, 'max': 4, 'palette': palette},\n",
    "    slide_img)\n",
    "\n",
    "mp.add_child(folium.LayerControl())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFuLna6zQ_zt"
   },
   "source": [
    "Now interpretation is somewhat easier. The negative definite (cyan) changes which appear between Nov. 7 and Nov. 13 correspond to decreases in intensity of _VV_ and _VH_ reflectance and are due to wide-spread flooding. The positive definite changes (red), which gradually overlay the flooded areas in subsequent intervals, correspond to receding flood waters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WhBsXEjsZCU"
   },
   "source": [
    "### Outlook\n",
    "\n",
    "Without reliable ground truth we can't really claim that change maps of the kind we have just generated will be helpful for flood damage assessment or control, but their potential usefulness is quite obvious. In the next and final part of the Tutorial we will have a look at some more (possible) applications of sequential change detection with SAR imagery using GEE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "BIG SUR Landslide Version of Detecting Changes in Sentinel-1 Imagery (Part 3)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
